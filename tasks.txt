* scale loss on the diagram
* add


Goals:
1. Get my hands on a well trained set of parameters to do transfer learning
    a. try the lower learning rate for Parikh model
    b. look at implementation details for errors
    c. do I want to investigate batch size?
2. Start working on encoding models
    a. keep reading papers
    b. implement promising ideas
3. Make the training process FASTER
    a. prepending NULL to be added into mongo
    *  already got the projection fetch in new repo method
    b. encoded label can be put into mongo
    c. review the padding out of sentences - can be by batch?  look at model details
    *  adding third dimension to batches necessary in code
    d. when I figure out what to do with OOV, make the change in mongo
    e. async buffer filling
4. Clean up my training process
    a. global step is not being saved properly

Training Process:
* Create separate checkpoint files based on config settings automatically
* Back up parameters in separate folders at certain times
* Add an option to look at accuracy on dev / train after each epoch and record that history
    - visualizing on TB may also be great
* Broader algorithm to assist training, saving results, visualizing, etc
* Delete old scalar data from summary so that I can actually look at a clean graph
    - or move back to matplotlib and just customize it all

Models:
* Assess performance on properly clean data set:
    - no gold labels removed
    - no word vectors removed
    - prepending null token

New DataSet:
* Download the new data set, process it, get it in there, build the infrastructure to be able to use it

Parkih:
- OOV words hashed to one of 100 random vectors, then projected
- 2 layers with 200 neurons
- dropout rate 0.2
- LR = 0.05

* Global step is still not being saved properly
* It would also be nice if the number of epochs were saved on the model
* I can define the batch length and then automatically calculate the number of iterations given the collection size