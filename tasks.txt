* Run process_data cleanup functions
    - THINK about making new properties instead of just overwriting everything

Make get_batch_gen more efficient:
* Project desired fields from mongo
* Shift as much of the processing out by saving already in the db
    - timesteps?  Might have to look through the code to see if I can fix this
* By deleting no gold label records I can remove the need to process that

Training Process:
* Create separate checkpoint files based on config settings automatically
* Back up parameters in separate folders at certain times
* Add an option to look at accuracy on dev / train after each epoch and record that history
    - visualizing on TB may also be great
* Broader algorithm to assist training, saving results, visualizing, etc
* Delete old scalar data from summary so that I can actually look at a clean graph
    - or move back to matplotlib and just customize it all

Models:
* Assess performance on properly clean data set:
    - no gold labels removed
    - no word vectors removed
    - prepending null token

New DataSet:
* Download the new data set, process it, get it in there, build the infrastructure to be able to use it

Parkih:
- OOV words hashed to one of 100 random vectors, then projected
- 2 layers with 200 neurons
- dropout rate 0.2
- LR = 0.05

* Global step is still not being saved properly
* It would also be nice if the number of epochs were saved on the model
* I can define the batch length and then automatically calculate the number of iterations given the collection size