Data Cleaning / Prep:
* Generate vocab dictionary
* Determine missing word vectors
* Train missing word vectors

Training Process:
* Create separate checkpoint files based on config settings automatically
* Add an option to look at accuracy on dev / train after each epoch and record that history
    - visualizing on TB may also be great
* Broader algorithm to assist training, saving results, visualizing, etc

Models:
* Assess performance on properly clean data set:
    - no gold labels removed
    - no word vectors removed
    - prepending null token

New DataSet:
* Download the new data set, process it, get it in there, build the infrastructure to be able to use it

Parkih:
- OOV words hashed to one of 100 random vectors, then projected
- 2 layers with 200 neurons
- dropout rate 0.2
- LR = 0.05

*** Got to bring computation time down
- get_batch limian de code can be moved into Mongo - pre-process Mofo
- Got to find a way to deal with variable lengths, timesteps...

* Global step is still not being saved properly
* It would also be nice if the number of epochs were saved on the model